""" domains either learned or constructed from other domains """

from typing import Callable, Tuple
import copy
import numpy as np
from typing_extensions import Protocol

from po_nrl.agents.neural_networks.neural_pomdps import DynamicsModel, get_optimizer_builder
from po_nrl.environments import ActionSpace, TerminalState
from po_nrl.environments import Simulator, SimulationResult
from po_nrl.misc import Space, DiscreteSpace, POBNRLogger
from po_nrl.pytorch_api import tensorboard_logging


class TransitionSampler(Protocol):
    """type to represent a transition sampler for learning environments"""
    def __call__(self) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
        """plain call to generate s,a,s',o sample"""


def sample_transitions_uniform_from_simulator(sim: Simulator) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
    """Samples transitions uniformly from simulator

    Samples state-action pairs uniformly from the environment (the next state
    and observations are generated by the simulator)

    Args:

        sim (Simulator):

    Returns:
        Tuple[np.ndarray, int, np.ndarray, np.ndarray]: transition (s,a,s',o)
    """

    while True:
        try:
            state = sim.state_space.sample()
            action = sim.action_space.sample()

            next_state, observation = sim.simulation_step(state, action)

            return state, action, next_state, observation

        except TerminalState:
            continue


def train_from_samples(
        model: DynamicsModel,
        sampler: TransitionSampler,
        num_epochs: int,
        batch_size: int) -> None:
    """ trains a model with data uniformly sampled from (S,A) space

    Performs `num_epochs` updates of size `batch_size` by sampling from sampler

    Args:
         model: (`po_nrl.agents.neural_networks.neural_pomdps.DynamicsModel`):
         sampler: (`TransitionSampler`): method to sample transitions from
         num_epochs: (`int`): number of batch updates
         batch_size: (`int`): size of a batch update

    RETURNS (`None`):

    """

    log_loss = tensorboard_logging()
    for _ in range(num_epochs):
        states, actions, new_states, observations = zip(*[sampler() for _ in range(batch_size)])
        model.batch_update(np.array(states), np.array(actions), np.array(new_states), np.array(observations), log_loss)


class NeuralEnsemblePOMDP(Simulator, POBNRLogger):
    """ A simulator over (`po_nrl.agents.neural_networks.neural_pomdps.DynamicsModel`, state) states """

    class AugmentedState:
        """ A state containing (POMDP state, POMDP dynamics) """

        def __init__(self, domain_state: np.ndarray, model: DynamicsModel):

            self.domain_state = domain_state
            self.model = model

        def __repr__(self) -> str:
            return f'Augmented state: state {self.domain_state} with model {self.model}'

    def __init__(self, domain: Simulator, conf):
        """ Creates `NeuralEnsemblePOMDP`

        Args:
             domain: (`po_nrl.environments.Simulator`): domain to train interactions from
             conf: configurations from program input (`network_size`, `learning_rate` and `dropout_rate`)

        """

        POBNRLogger.__init__(self)

        # domain knowledge
        self.domain_action_space = domain.action_space
        self.domain_obs_space = domain.observation_space

        assert isinstance(self.domain_obs_space, DiscreteSpace),\
            f"current limited to learning discrete POMDPs, not {domain}"
        assert isinstance(domain.state_space, DiscreteSpace),\
            f"current limited to learning discrete POMDPs, not {domain}"

        self.sample_domain_start_state = domain.sample_start_state

        self.domain_reward = domain.reward
        self.domain_terminal = domain.terminal

        optimizer_builder = get_optimizer_builder(conf.optimizer)

        self._models = [
            DynamicsModel(
                domain.state_space,
                domain.action_space,
                self.domain_obs_space,
                conf.network_size,
                conf.learning_rate,
                conf.batch_size,
                conf.dropout_rate,
                optimizer_builder
            ) for i in range(conf.num_nets)
        ]

    @property
    def state_space(self) -> Space:
        """ No method should want to know `this` state space, raises error """
        raise NotImplementedError

    def __len__(self):
        return len(self._models)

    @property
    def action_space(self) -> ActionSpace:
        """ returns `this` actions space

        Args:

        RETURNS (`po_nrl.environments.ActionSpace`):

        """
        return self.domain_action_space

    @property
    def observation_space(self) -> DiscreteSpace:
        """ returns `this` observation space

        Args:

        RETURNS (`po_nrl.misc.Space`):

        """
        assert isinstance(self.domain_obs_space, DiscreteSpace),\
            f"current limited to learning discrete POMDPs, not {self.domain_obs_space}"
        return self.domain_obs_space

    def sample_start_state(self) -> 'AugmentedState':
        """  returns a sample initial (internal) state and some neural network

        Args:

        RETURNS (`AugmentedState`):

        """
        return self.AugmentedState(
            self.sample_domain_start_state(),
            copy.deepcopy(np.random.choice(self._models))
        )

    def simulation_step(
            self,
            state: AugmentedState,
            action: int) -> SimulationResult:
        """ Performs simulation step

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`po_nrl.environments.SimulationResult`):

        """

        # use model to generate a step
        new_domain_state, obs = state.model.simulation_step(state.domain_state, action)

        return SimulationResult(self.AugmentedState(new_domain_state, state.model), obs)

    def reward(self, state: AugmentedState, action: int, new_state: AugmentedState) -> float:
        """ the reward function of the underlying environment

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             new_state: (`AugmentedState`):

        RETURNS (`float`): the reward of the transition

        """
        return self.domain_reward(state.domain_state, action, new_state.domain_state)

    def terminal(self, state: AugmentedState, action: int, new_state: AugmentedState) -> bool:
        """ the termination function of the underlying environment

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             new_state: (`AugmentedState`):

        RETURNS (`bool`): whether the transition is terminal

        """
        return self.domain_terminal(state.domain_state, action, new_state.domain_state)

    def reset(self, train_net_f: Callable[[DynamicsModel], None], learning_rate: float, online_learning_rate: float) -> None:
        """ resets the ensemble and re-trains them according

        After training, the learning rate is reset to `online_learning_rate`
        for online updating

        Args:
             train_net_f: (`Callable[[DynamicsModel], None]`):
             learning_rate: (`float`): learning rate to learn models with
             online_learning_rate: (`float`): learning rate to set to after learning

        RETURNS (`None`):

        """

        for i, model in enumerate(self._models):

            self.log(
                POBNRLogger.LogLevel.V1,
                f'Training Dynamics Neural Ensemble member {i+1}/{len(self._models)}'
            )

            model.reset()
            model.set_learning_rate(learning_rate)
            train_net_f(model)

            # after training models we use this low learning rate
            # for online updates
            model.set_learning_rate(online_learning_rate)
