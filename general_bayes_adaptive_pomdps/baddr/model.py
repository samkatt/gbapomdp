import copy
from logging import Logger
from typing import Callable, Tuple, List
import numpy as np
from general_bayes_adaptive_pomdps.baddr.neural_networks.neural_pomdps import (
    DynamicsModel,
    get_optimizer_builder,
)
from general_bayes_adaptive_pomdps.core import (
    GeneralBAPOMDP,
    ActionSpace,
    Domain,
    SimulationResult,
    TerminalState,
)
from general_bayes_adaptive_pomdps.misc import DiscreteSpace, LogLevel, Space
from functools import partial
from typing_extensions import Protocol
from general_bayes_adaptive_pomdps.domains.gridverse_domain import GridverseDomain


class TransitionSampler(Protocol):
    """type to represent a domain transition sampler"""

    def __call__(self) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
        """plain call to generate s,a,s',o sample"""


def train_from_samples(
    theta: DynamicsModel,
    sampler: TransitionSampler,
    num_epochs: int,
    batch_size: int,
) -> None:
    """trains a theta with data uniformly sampled from (S,A) space

    Performs `num_epochs` updates of size `batch_size` by sampling from sampler

    Args:
         theta: (`general_bayes_adaptive_pomdps.baddr.neural_networks.neural_pomdps.DynamicsModel`):
         sampler: (`TransitionSampler`): method to sample transitions from
         num_epochs: (`int`): number of batch updates
         batch_size: (`int`): size of a batch update

    RETURNS (`None`):
    """

    for _ in range(num_epochs):
        states, actions, new_states, observations = zip(
            *[sampler() for _ in range(batch_size)]
        )
        theta.batch_update(
            np.array(states),
            np.array(actions),
            np.array(new_states),
            np.array(observations),
        )


def sample_transitions_uniform_from_simulator(
    sim: Domain,
) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
    """Samples transitions uniformly from simulator

    Samples state-action pairs uniformly from the domain (the next state
    and observations are generated by the simulator)

    Args:

        sim (Simulator):

    Returns:
        Tuple[np.ndarray, int, np.ndarray, np.ndarray]: transition (s,a,s',o)
    """

    while True:
        try:
            state = sim.state_space.sample()
            action = sim.action_space.sample_as_int()

            next_state, observation = sim.simulation_step(state, action)

            return state, action, next_state, observation

        except TerminalState:
            continue


def sample_from_gridverse(
    d: GridverseDomain,
) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
    """Generates the sampler for the Gridverse problem

    Uses the 'reset' function of Gridverse domain, but then randomly positions
    the agent, to ensure the whole state space is visited

    The implementation involves defining a state-sampler from `d` and then
    using the Gridverse library build sampling function

    Args:
        d (`GridverseDomain`):

    Returns:
        Tuple[np.ndarray, int, np.ndarray, np.ndarray]: (s,a,s',o)
    """
    return d.sample_transition()


def create_transition_sampler(sim: Domain) -> TransitionSampler:
    """factory method for generating transition samplers

    Basically returns the correct sampler, at this point there are really 2
    possibilities: either a Gridverse-specific sampler, or the uniform
    state-action sampler is returned

    Args:
        sim (`Simulator`):

    Returns:
        `TransitionSampler`:
    """
    if isinstance(sim, GridverseDomain):
        return partial(sample_from_gridverse, d=sim)

    return partial(sample_transitions_uniform_from_simulator, sim=sim)


def create_dynamics_model(
    domain: Domain,
    optimizer: str,
    learning_rate: float,
    network_size: int,
    batch_size: int,
    dropout_rate: float = 0.0,
    known_model: str = "",
) -> DynamicsModel:
    """factory function for creating `DynamicsModel`

    The `DynamicsModel` consists of a transition and observation model. Either
    of those can be a neural network, or a (known) function. The model itself
    cares little about what its models are like. This function will assemble
    the correct ones according to the configuration

    Args:
        domain (`Simulator`):
        optimizer (`str`): in ["SGD" or "Adam"], sets type of optimizer
        learning_rate (`float`):
        network_size (`int`): number of nodes in each fully connected layer
        batch_size (`int`):
        dropout_rate (`float`): ratio of dropping nodes (default is 0)
        known_model (`str`): defaults to ""

    Returns:
        `DynamicsModel`:
    """

    def create_o_model() -> DynamicsModel.ObsModel:
        if known_model != "O":

            assert isinstance(
                domain.observation_space, DiscreteSpace
            ), "This method assumes discrete spaces"

            assert isinstance(
                domain.state_space, DiscreteSpace
            ), "This method assumes discrete spaces"

            return DynamicsModel.ONet(
                domain.state_space,
                domain.action_space,
                domain.observation_space,
                optimizer_builder,
                learning_rate,
                network_size,
                dropout_rate,
            )

        if not isinstance(domain, GridverseDomain):
            raise ValueError(
                "Currently no support for known models for domains other than gridverse"
            )

        assert isinstance(domain, GridverseDomain)
        return domain.create_dynamics_observation_model()

    def create_t_model() -> DynamicsModel.T:
        if known_model != "T":

            assert isinstance(
                domain.state_space, DiscreteSpace
            ), "This method assumes discrete spaces"

            return DynamicsModel.TNet(
                domain.state_space,
                domain.action_space,
                optimizer_builder,
                learning_rate,
                network_size,
                dropout_rate,
            )

        raise ValueError("Currently no support for known models")

    assert isinstance(
        domain.state_space, DiscreteSpace
    ), "This method assumes discrete spaces"

    optimizer_builder = get_optimizer_builder(optimizer)
    return DynamicsModel(
        domain.state_space,
        domain.action_space,
        batch_size,
        create_t_model(),
        create_o_model(),
    )


class ModelUpdate(Protocol):
    """Defines the signature of a model update function

    These functions are used to **augment** the importance sampling update.
    I.e. this function will enable the model to be updated

    """

    def __call__(
        self,
        model: DynamicsModel,
        state: np.ndarray,
        action: int,
        next_state: np.ndarray,
        observation: np.ndarray,
    ) -> None:
        pass


def get_model_freeze_setting(
    freeze_model: str,
) -> DynamicsModel.FreezeModelSetting:
    """returns the model freeze setting given configuration string

    Args:
         freeze_model: (`str`):

     Returns:
         `general_bayes_adaptive_pomdps.baddr.neural_networks.neural_pomdps.DynamicsModel.FreezeModelSetting`:

    """
    if not freeze_model:
        return DynamicsModel.FreezeModelSetting.FREEZE_NONE

    if freeze_model == "T":
        return DynamicsModel.FreezeModelSetting.FREEZE_T

    if freeze_model == "O":
        return DynamicsModel.FreezeModelSetting.FREEZE_O

    raise ValueError("Wrong value given to freeze model argument")


def backprop_update(
    model: DynamicsModel,
    state: np.ndarray,
    action: int,
    next_state: np.ndarray,
    observation: np.ndarray,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """A type of belief update: applies a simple backprop call to the model

    Args:
         model: (`DynamicsModel`):
         state: (`np.ndarray`):
         action: (`np.ndarray`):
         next_state: (`np.ndarray`):
         observation: (`np.ndarray`):
         freeze_model_setting: (`FreezeModelSetting`)

    Returns:
        None
    """

    # `batch_update` expects batch_size x ... size. [None] adds a dimension
    model.batch_update(
        state[None],
        np.array([action]),
        next_state[None],
        observation[None],
        freeze_model_setting,
    )


def replay_buffer_update(
    model: DynamicsModel,
    state: np.ndarray,
    action: int,
    next_state: np.ndarray,
    observation: np.ndarray,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """will add transition to model and then invoke a `self learn` step

    Args:
         model: (`DynamicsModel`):
         state: (`np.ndarray`):
         action: (`np.ndarray`):
         next_state: (`np.ndarray`):
         observation: (`np.ndarray`):
         freeze_model_setting: (`FreezeModelSetting`)

    Returns:
        None
    """

    model.add_transition(state, action, next_state, observation)
    model.self_learn(freeze_model_setting)


def perturb_parameters(
    model: DynamicsModel,
    state: np.ndarray,
    action: int,
    next_state: np.ndarray,
    observation: np.ndarray,
    stdev: float,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """A type of belief update: applies gaussian noise to model parameters

    Args:
         model: (`DynamicsModel`):
         _: (`np.ndarray`): ignored
         __: (`np.ndarray`): ignored
         ___: (`np.ndarray`): ignored
         _____: (`np.ndarray`): ignored
         stdev: (`float`): the standard deviation of the applied noise
         freeze_model_setting: (`FreezeModelSetting`)

    RETURNS (`None`):

    """

    model.perturb_parameters(stdev, freeze_model_setting)


def create_model_updates(
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
    backprop: bool,
    replay_update: bool,
    perturb_stdev: float,
) -> List[ModelUpdate]:
    """create_model_updates.

    Args:
        freeze_model_setting (`DynamicsModel.FreezeModelSetting`):
        backprop (`bool`): whether to do backpropagation
        replay_update (`bool`): whether to use a replay buffer to train on
        perturb_stdev (`float`): amount to perturb parameters by during update

    Returns:
        `List[ModelUpdate]`:
    """
    updates: List[ModelUpdate] = []

    if backprop:
        updates.append(
            partial(backprop_update, freeze_model_setting=freeze_model_setting)
        )
    if replay_update:
        updates.append(
            partial(
                replay_buffer_update,
                freeze_model_setting=freeze_model_setting,
            )
        )
    if perturb_stdev:
        updates.append(
            partial(
                perturb_parameters,
                stdev=perturb_stdev,
                freeze_model_setting=freeze_model_setting,
            )
        )

    return updates


class BADDr(Domain, GeneralBAPOMDP):
    """A simulator over augmented states

    The augmented states are
    (`general_bayes_adaptive_pomdps.baddr.neural_networks.neural_pomdps.DynamicsModel`, state) tuples
    """

    class AugmentedState:
        """ A state containing (POMDP state, POMDP dynamics) """

        def __init__(self, domain_state: np.ndarray, model: DynamicsModel):

            self.domain_state = domain_state
            self.model = model

        def __repr__(self) -> str:
            return f"Augmented state: state {self.domain_state} with model {self.model}"

    def __init__(
        self,
        domain: Domain,
        num_nets: int,
        optimizer: str,
        learning_rate: float,
        network_size: int,
        batch_size: int,
        dropout_rate: float = 0.0,
        known_model: str = "",
        freeze_model: str = "",
        backprop: bool = True,
        replay_update: bool = False,
        perturb_stdev: float = 0.0,
    ):
        """Creates `BADDr`

        Some say "limit the number of constructor arguments", others like to be dumb.

        TODO: reduce # arguments
        TODO: set device

        Args:
             domain: (`general_bayes_adaptive_pomdps.core.Simulator`): domain to train interactions from
             num_nets: (`int`):
             optimizer: (`str`):
             learning_rate: (`float`):
             network_size: (`int`):
             batch_size: (`int`):
             dropout_rate: (`float`): defaults to 0.0
             known_model: (`str`): defaults to ""
             freeze_model: (`str`): defaults to ""
             backprop: (`bool`): defaults to True
             replay_update: (`bool`): defaults to False
             perturb_stdev: (`float`): defaults to 0.0

        """

        self._logger = Logger(self.__class__.__name__)

        # domain knowledge
        self.domain_action_space = domain.action_space
        self.domain_obs_space = domain.observation_space

        assert isinstance(
            self.domain_obs_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {domain}"
        assert isinstance(
            domain.state_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {domain}"

        self.sample_domain_start_state = domain.sample_start_state

        self.domain_reward = domain.reward
        self.domain_terminal = domain.terminal

        self.domain_state_to_string = domain.state_to_string
        self.domain_action_to_string = domain.action_to_string
        self.domain_observation_to_string = domain.observation_to_string

        self._models = [
            create_dynamics_model(
                domain,
                optimizer,
                learning_rate,
                network_size,
                batch_size,
                dropout_rate,
                known_model,
            )
            for _ in range(num_nets)
        ]

        self._theta_updates = create_model_updates(
            get_model_freeze_setting(freeze_model),
            backprop,
            replay_update,
            perturb_stdev,
        )

    @property
    def state_space(self) -> Space:
        """ No method should want to know `this` state space, raises error """
        raise NotImplementedError

    def __len__(self):
        return len(self._models)

    @property
    def action_space(self) -> ActionSpace:
        """returns `this` actions space

        Args:

        RETURNS (`general_bayes_adaptive_pomdps.core.ActionSpace`):

        """
        return self.domain_action_space

    @property
    def observation_space(self) -> DiscreteSpace:
        """returns `this` observation space

        Args:

        RETURNS (`general_bayes_adaptive_pomdps.misc.Space`):

        """
        assert isinstance(
            self.domain_obs_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {self.domain_obs_space}"
        return self.domain_obs_space

    def sample_start_state(self) -> "AugmentedState":
        """returns a sample initial (internal) state and some neural network

        Args:

        RETURNS (`AugmentedState`):
        """
        return self.AugmentedState(
            self.sample_domain_start_state(),
            copy.deepcopy(np.random.choice(self._models)),
        )

    def update_theta(
        self,
        theta: DynamicsModel,
        state: np.ndarray,
        action: np.ndarray,
        next_state: np.ndarray,
        observation: np.ndarray,
    ) -> None:
        """applies all parameters updates on theta, given transition

        Construction figures out what updates are applied onto the parameters
        ``theta`` during a step in the GBA-POMDP. Here, those updates are
        applied given the (``state``, ``action``, ``next_state``,
        ``observation``) transition.

        These updates are applied in place, thus ``theta`` is updated

        Args:
             theta: (`DynamicsModel`):
             state: (`np.ndarray`):
             action: (`np.ndarray`):
             next_state: (`np.ndarray`):
             observation: (`np.ndarray`):

        RETURNS (`None`):

        """
        for update in self._theta_updates:
            update(theta, state, action, next_state, observation)

    @staticmethod
    def simulate_domain_step(
        theta: DynamicsModel, domain_state: np.ndarray, action: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Performs simulation step of the domain state

        Uses ``theta`` as theta to simulate a step in the domain given the
        ``domain_state`` and ``action``. Returns the resulting new domain state
        and observation (both ``np.ndarray``)

        Args:
             theta: (`DynamicsModel`): model parameters
             domain_state: (`np.ndarray`): state at t
             action: (`int`): action at t

        RETURNS (`Tuple[np.ndarray, np.ndarray]`): (s, o) at t+1

        """

        # use theta to generate a step
        return theta.simulation_step(domain_state, action)

    def simulation_step(self, state: "AugmentedState", action: int) -> SimulationResult:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP:

        - updates domain state according to :meth:`~simulate_domain_step`
        - updates theta according to :meth:`~update_theta`
        - COPIES MODEL, if you do not care about modifying ``state``, see :meth:`simulation_step_inplace`

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`general_bayes_adaptive_pomdps.core.SimulationResult`):

        """

        # ``simulation_step_inplace`` modifies the model in ``state``, copy and store it
        old_model = copy.deepcopy(state.model)

        # the actual step
        ret = self.simulation_step_inplace(state, action)

        # both ``state`` and the state in ``ret`` refer to the same updated model
        # reset the original model to the incoming copied one
        state.model = old_model

        return ret

    def simulation_step_inplace(
        self, state: "AugmentedState", action: int
    ) -> SimulationResult:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP:

        - updates domain state according to :meth:`~simulate_domain_step`
        - updates theta according to :meth:`~update_theta`
        - MODIFIES ``state``

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`general_bayes_adaptive_pomdps.core.SimulationResult`):

        """

        new_domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        # update model of state in place
        self.update_theta(
            state.model, state.domain_state, action, new_domain_state, obs
        )

        # return new state with reference to old model
        return SimulationResult(self.AugmentedState(new_domain_state, state.model), obs)

    def simulation_step_without_updating_theta(
        self, state: "AugmentedState", action: int
    ) -> SimulationResult:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP
        without actually updating the theta parameters in ``state``:

        - updates domain state according to :meth:`~simulate_domain_step`
        - updates theta according to :meth:`~update_theta`

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`general_bayes_adaptive_pomdps.core.SimulationResult`):

        """

        new_domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        return SimulationResult(self.AugmentedState(new_domain_state, state.model), obs)

    def reward(
        self, state: "AugmentedState", action: int, new_state: "AugmentedState"
    ) -> float:
        """the reward function of the underlying domain

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             new_state: (`AugmentedState`):

        RETURNS (`float`): the reward of the transition

        """
        return self.domain_reward(state.domain_state, action, new_state.domain_state)

    def terminal(
        self, state: "AugmentedState", action: int, new_state: "AugmentedState"
    ) -> bool:
        """the termination function of the underlying domain

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             new_state: (`AugmentedState`):

        RETURNS (`bool`): whether the transition is terminal

        """
        return self.domain_terminal(state.domain_state, action, new_state.domain_state)

    def reset(
        self,
        train_net_f: Callable[[DynamicsModel], None],
        learning_rate: float,
        online_learning_rate: float,
    ) -> None:
        """resets the ensemble and re-trains them according

        After training, the learning rate is reset to `online_learning_rate`
        for online updating

        Args:
             train_net_f: (`Callable[[DynamicsModel], None]`):
             learning_rate: (`float`): learning rate to learn models with
             online_learning_rate: (`float`): learning rate to set to after learning

        RETURNS (`None`):

        """

        for i, model in enumerate(self._models):

            self._logger.log(
                LogLevel.V1.value,
                f"Training Dynamics Neural Ensemble member {i+1}/{len(self._models)}",
            )

            model.reset()
            model.set_learning_rate(learning_rate)
            train_net_f(model)

            # after training models we use this low learning rate
            # for online updates
            model.set_learning_rate(online_learning_rate)

    def state_to_string(self, state: "AugmentedState") -> str:
        """Returns a string representation of the `state`

        Prints underlying state

        Args:
            state (`AugmentedState`):

        Returns:
            `str`:
        """
        return f"Augmented state {self.domain_state_to_string(state.domain_state)}"

    def action_to_string(self, action: int) -> str:
        """Returns a string representation of the `action`

        Prints domain action

        Args:
            action (`int`):

        Returns:
            `str`:
        """
        return self.domain_action_to_string(action)

    def observation_to_string(self, observation: np.ndarray) -> str:
        """Returns a string representation of the `observation`

        Prints domain observation

        Args:
            observation (`np.ndarray`):

        Returns:
            `str`:
        """
        return self.domain_observation_to_string(observation)

    def domain_simulation_step(
        self, state: "AugmentedState", action: int
    ) -> SimulationResult:
        """Performs the domain state part of the step in the GBA-POMDP

        Leaves the model alone

        Note: part of `GBAPOMDP` protocol
        """
        domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        next_state = self.AugmentedState(domain_state, state.model)

        return SimulationResult(next_state, obs)

    def model_simulation_step(
        self,
        to_update: "AugmentedState",
        prev_state: "AugmentedState",
        action: int,
        next_state: "AugmentedState",
        obs: np.ndarray,
        optimize: bool = False,
    ) -> "AugmentedState":
        """Performs the model part of the step in the GBA-POMDP
        Leaves the state alone
        If ``optimize`` is true, will modify model in ``to_update``
        Note: part of `GBAPOMDP` protocol
        """
        model = to_update.model if optimize else copy.deepcopy(to_update.model)

        self.update_theta(
            model, prev_state.domain_state, action, next_state.domain_state, obs
        )
        return self.AugmentedState(to_update.domain_state, model)
