"""The definition of a general Bayes adaptive POMDP

Provides the protocol for the main component of the whole package: the general
Bayes-adaptive POMDP.
"""

from typing import TypeVar

import numpy as np
from typing_extensions import Protocol

from general_bayes_adaptive_pomdps.environments import ActionSpace, SimulationResult
from general_bayes_adaptive_pomdps.misc import DiscreteSpace

AugmentedState = TypeVar("AugmentedState")
"""The state space of a General Bayes-adaptive POMDP"""


class GBAPOMDP(Protocol[AugmentedState]):
    """Defines the protocol of a general Bayes-adaptive POMDP

    The GBA-POMDP _is a_ POMDP with a special state space. It augments an
    existing POMDP, of which the dynamics are unknown, and constructs a larger
    POMDP with known dynamics. Here we describe the generally assumed API.

    The class is templated by the state space
    """

    @property
    def action_space(self) -> ActionSpace:
        """The space of legal actions in this GBA-POMDP

        This space should be the same as the action space of the underlying POMDP

        :return: legal actions in ``self``
        """

    @property
    def observation_space(self) -> DiscreteSpace:
        """The space of possible observations in this GBA-POMDP

        This space should be the same as the observation space of the underlying POMDP

        Note that this space may be much larger than the actual observations
        found during interactions in the (GBA-) POMDP.

        :return: space of possible observations
        """

    def sample_start_state(self) -> AugmentedState:
        """Basic functionality of POMDPs: returns an initial state

        This typical is some combination of a prior belief over the model of
        the underlying POMDP and an initial state in the underlying POMDP.

        :return: initial state in the GBA-POMDP
        """
        ...

    def simulation_step(
        self, state: AugmentedState, action: int, optimize: bool = False
    ) -> SimulationResult:
        """Performs an actual step according to the GBA-POMDP dynamics

        The resulting `SimulationResult` contains the generated state and observation, where the state is generated by:

            1. sample a new domain state and observation according to the belief over the model in ``state``
            2. update model in ``state`` according to sampled transition

        Note that this operation is expensive, as it often involves generating
        a new set of parameters to represent the model distribution in the new
        state (which involves a copy before updating). If there is no need for
        the input ``state`` to stay unmodified, consider calling
        `domain_simulation_step` and `model_simulation_step` to achieve the
        same without the copy.

        :param state: state at timestep t
        :param action: action at timestep t
        :param optimize: optimization flag.
            If set to true, the model in ``state`` is _not_ copied, and
            thus the incoming ``state`` _is modified_. If there is no need
            to keep the old model, then setting this flag skips a then needless
            copy operation, which can be significant
        :return: (state, obs) at timestep t+1
        """

    def domain_simulation_step(
        self, state: AugmentedState, action: int
    ) -> SimulationResult:
        """Performs the domain state part of the step in the GBA-POMDP

        The typical step updates both the state and the model. For some
        operations it is useful to break up these two steps. This function
        contains the first step. It will sample a next state and observation
        according to the model distribution in ``state`` and return those.

        NOTE: that the model in the returned state is a reference to the one in
        ``state`` so be careful

        :param state: state at timestep t
        :param action: aciton at timestep t
        :return: (state, obs) at t+1, where model in state == of the incoming ``state``
        """

    def model_simulation_step(
        self,
        to_update: AugmentedState,
        prev_state: AugmentedState,
        action: int,
        next_state: AugmentedState,
        obs: np.ndarray,
        optimize: bool = False,
    ) -> AugmentedState:
        """Performs the model part of the step in the GBA-POMDP

        The typical step updates both the state and the model. For some
        operations it is useful to break up these two steps. This function
        contains the second step. It will update the model in ``to_update``
        according to the transition (``prev_state``, ``action``,
                ``next_state``, ``obs``) and return a new augmented state with
        that model.

        NOTE: the domain state in the returned state is a reference to the one in
        ``to_update`` so be careful.

        :param to_update: the augmented state that contains the model at timestep t
        :param prev_state: the state at timestep t of the transition
        :param action: the action at timestep t of the transition
        :param next_state: the state at timestep t+1 of the transition
        :param obs: the observation at timestep t of the transition
        :param optimize: optimization flag.
            If set to true, the model in ``to_update`` is _not_ copied, and
            thus the incoming ``to_update`` _is modified_. If there is no need
            to keep the old model, then setting this flag skips a then needless
            copy operation, which can be significant
        :return: new state with updated model (same domain state as ``to_update``)
        """

    def reward(
        self, state: AugmentedState, action: int, next_state: AugmentedState
    ) -> float:
        """computes the reward associated with transition

        The reward function, typically calls the (known) reward function of the
        underlying POMDP with the domain states in the (s,a,s') transition

        :param state: state at timestep t
        :param action: action at timestep t
        :param next_state: state at timestep t+1
        :return: reward generated in input transition
        """

    def terminal(
        self, state: AugmentedState, action: int, next_state: AugmentedState
    ) -> bool:
        """computes whether the input transition is terminal

        The terminality function, typically calls the (known) terminality
        function of the underlying POMDP with the domain states in the
        (s,a,s') transition

        :param state: state at timestep t
        :param action: action at timestep t
        :param next_state: state at timestep t+1
        :return: true is the input transition was terminal
        """
