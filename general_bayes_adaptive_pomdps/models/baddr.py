import copy
from functools import partial
from typing import Callable, List, Tuple

import numpy as np
from typing_extensions import Protocol

from general_bayes_adaptive_pomdps.agents.neural_networks.neural_pomdps import (
    DynamicsModel,
    get_optimizer_builder,
)
from general_bayes_adaptive_pomdps.domains.gbapomdp import GBAPOMDP
from general_bayes_adaptive_pomdps.domains.gridverse_domain import GridverseDomain
from general_bayes_adaptive_pomdps.environments import (
    ActionSpace,
    SimulationResult,
    Simulator,
    TerminalState,
)
from general_bayes_adaptive_pomdps.misc import DiscreteSpace, POBNRLogger, Space
from general_bayes_adaptive_pomdps.pytorch_api import tensorboard_logging


class TransitionSampler(Protocol):
    """type to represent a transition sampler for learning environments"""

    def __call__(self) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
        """plain call to generate s,a,s',o sample"""


def train_from_samples(
    theta: DynamicsModel,
    sampler: TransitionSampler,
    num_epochs: int,
    batch_size: int,
) -> None:
    """trains a theta with data uniformly sampled from (S,A) space

    Performs `num_epochs` updates of size `batch_size` by sampling from sampler

    Args:
         theta: (`general_bayes_adaptive_pomdps.agents.neural_networks.neural_pomdps.DynamicsModel`):
         sampler: (`TransitionSampler`): method to sample transitions from
         num_epochs: (`int`): number of batch updates
         batch_size: (`int`): size of a batch update

    RETURNS (`None`):
    """

    log_loss = tensorboard_logging()
    for _ in range(num_epochs):
        states, actions, new_states, observations = zip(
            *[sampler() for _ in range(batch_size)]
        )
        theta.batch_update(
            np.array(states),
            np.array(actions),
            np.array(new_states),
            np.array(observations),
            log_loss,
        )


def sample_transitions_uniform_from_simulator(
    sim: Simulator,
) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
    """Samples transitions uniformly from simulator

    Samples state-action pairs uniformly from the environment (the next state
    and observations are generated by the simulator)

    Args:

        sim (Simulator):

    Returns:
        Tuple[np.ndarray, int, np.ndarray, np.ndarray]: transition (s,a,s',o)
    """

    while True:
        try:
            state = sim.state_space.sample()
            action = sim.action_space.sample()

            next_state, observation = sim.simulation_step(state, action)

            return state, action, next_state, observation

        except TerminalState:
            continue


def sample_from_gridverse(
    d: GridverseDomain,
) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:
    """Generates the sampler for the Gridverse environments

    Uses the 'reset' function of Gridverse domain, but then randomly positions
    the agent, to ensure the whole state space is visited

    The implementation involves defining a state-sampler from `d` and then
    using the Gridverse library build sampling function

    Args:
        d (`GridverseDomain`):

    Returns:
        Tuple[np.ndarray, int, np.ndarray, np.ndarray]: (s,a,s',o)
    """
    return d.sample_transition()


def create_transition_sampler(sim: Simulator) -> TransitionSampler:
    """factory method for generating transition samplers

    Basically returns the correct sampler, at this point there are really 2
    possibilities: either a Gridverse-specific sampler, or the uniform
    state-action sampler is returned

    Args:
        sim (`Simulator`):

    Returns:
        `TransitionSampler`:
    """
    if isinstance(sim, GridverseDomain):
        return partial(sample_from_gridverse, d=sim)

    return partial(sample_transitions_uniform_from_simulator, sim=sim)


def create_dynamics_model(
    domain: Simulator,
    conf,
) -> DynamicsModel:
    """factory function for creating `DynamicsModel`

    The `DynamicsModel` consists of a transition and observation model. Either
    of those can be a neural network, or a (known) function. The model itself
    cares little about what its models are like. This function will assemble
    the correct ones according to the configuration

    Args:
        domain (`Simulator`):
        conf (namespace):

    Returns:
        `DynamicsModel`:
    """

    def create_o_model() -> DynamicsModel.ObsModel:
        if conf.known_model != "O":

            assert isinstance(
                domain.observation_space, DiscreteSpace
            ), "This method assumes discrete spaces"

            assert isinstance(
                domain.state_space, DiscreteSpace
            ), "This method assumes discrete spaces"

            return DynamicsModel.ONet(
                domain.state_space,
                domain.action_space,
                domain.observation_space,
                optimizer_builder,
                conf.learning_rate,
                conf.network_size,
                conf.dropout_rate,
            )

        if conf.domain != "gridverse":
            raise ValueError(
                "Currently no support for known models for domains other than gridverse"
            )

        assert isinstance(domain, GridverseDomain)
        return domain.create_dynamics_observation_model()

    def create_t_model() -> DynamicsModel.T:
        if conf.known_model != "T":

            assert isinstance(
                domain.state_space, DiscreteSpace
            ), "This method assumes discrete spaces"

            return DynamicsModel.TNet(
                domain.state_space,
                domain.action_space,
                optimizer_builder,
                conf.learning_rate,
                conf.network_size,
                conf.dropout_rate,
            )

        raise ValueError("Currently no support for known models")

    assert isinstance(
        domain.state_space, DiscreteSpace
    ), "This method assumes discrete spaces"

    optimizer_builder = get_optimizer_builder(conf.optimizer)
    return DynamicsModel(
        domain.state_space,
        domain.action_space,
        conf.batch_size,
        create_t_model(),
        create_o_model(),
    )


class ModelUpdate(Protocol):
    """Defines the signature of a model update function

    These functions are used to **augment** the importance sampling update.
    I.e. this function will enable the model to be updated

    """

    def __call__(
        self,
        model: DynamicsModel,
        state: np.ndarray,
        action: np.ndarray,
        next_state: np.ndarray,
        observation: np.ndarray,
    ) -> None:
        pass


def get_model_freeze_setting(
    freeze_model: str,
) -> DynamicsModel.FreezeModelSetting:
    """returns the model freeze setting given configuration string

    Args:
         freeze_model: (`str`):

    RETURNS ( `general_bayes_adaptive_pomdps.agents.neural_networks.neural_pomdps.DynamicsModel.FreezeModelSetting`):

    """
    if not freeze_model:
        return DynamicsModel.FreezeModelSetting.FREEZE_NONE

    if freeze_model == "T":
        return DynamicsModel.FreezeModelSetting.FREEZE_T

    if freeze_model == "O":
        return DynamicsModel.FreezeModelSetting.FREEZE_O

    raise ValueError("Wrong value given to freeze model argument")


def backprop_update(
    model: DynamicsModel,
    state: np.ndarray,
    action: np.ndarray,
    next_state: np.ndarray,
    observation: np.ndarray,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """A type of belief update: applies a simple backprop call to the model

    Args:
         model: (`DynamicsModel`):
         state: (`np.ndarray`):
         action: (`np.ndarray`):
         next_state: (`np.ndarray`):
         observation: (`np.ndarray`):
         freeze_model_setting: (`FreezeModelSetting`)

    RETURNS (`None`):

    """

    log_loss = False  # online we do not log the loss
    # `batch_update` expects batch_size x ... size. [None] adds a dimension
    model.batch_update(
        state[None],
        action[None],
        next_state[None],
        observation[None],
        log_loss,
        freeze_model_setting,
    )


def replay_buffer_update(
    model: DynamicsModel,
    state: np.ndarray,
    action: np.ndarray,
    next_state: np.ndarray,
    observation: np.ndarray,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """will add transition to model and then invoke a `self learn` step

    Args:
         model: (`DynamicsModel`):
         state: (`np.ndarray`):
         action: (`np.ndarray`):
         next_state: (`np.ndarray`):
         observation: (`np.ndarray`):
         freeze_model_setting: (`FreezeModelSetting`)

    RETURNS (`None`):

    """

    model.add_transition(state, action, next_state, observation)
    model.self_learn(freeze_model_setting)


def perturb_parameters(
    model: DynamicsModel,
    state: np.ndarray,
    action: np.ndarray,
    next_state: np.ndarray,
    observation: np.ndarray,
    stdev: float,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """A type of belief update: applies gaussian noise to model parameters

    Args:
         model: (`DynamicsModel`):
         _: (`np.ndarray`): ignored
         __: (`np.ndarray`): ignored
         ___: (`np.ndarray`): ignored
         _____: (`np.ndarray`): ignored
         stdev: (`float`): the standard deviation of the applied noise
         freeze_model_setting: (`FreezeModelSetting`)

    RETURNS (`None`):

    """

    model.perturb_parameters(stdev, freeze_model_setting)


def create_model_updates(conf) -> List[ModelUpdate]:  # set model update method
    freeze_model_setting = get_model_freeze_setting(conf.freeze_model)
    updates: List[ModelUpdate] = []

    if conf.backprop:
        updates.append(
            partial(backprop_update, freeze_model_setting=freeze_model_setting)
        )
    if conf.replay_update:
        updates.append(
            partial(
                replay_buffer_update,
                freeze_model_setting=freeze_model_setting,
            )
        )
    if conf.perturb_stdev:
        updates.append(
            partial(
                perturb_parameters,
                stdev=conf.perturb_stdev,
                freeze_model_setting=freeze_model_setting,
            )
        )

    return updates


class BADDr(Simulator, POBNRLogger, GBAPOMDP):
    """A simulator over augmented states

    The augmented states are
    (`general_bayes_adaptive_pomdps.agents.neural_networks.neural_pomdps.DynamicsModel`, state) tuples
    """

    class AugmentedState:
        """ A state containing (POMDP state, POMDP dynamics) """

        def __init__(self, domain_state: np.ndarray, model: DynamicsModel):

            self.domain_state = domain_state
            self.model = model

        def __repr__(self) -> str:
            return f"Augmented state: state {self.domain_state} with model {self.model}"

    def __init__(self, domain: Simulator, conf):
        """Creates `BADDr`

        Args:
             domain: (`general_bayes_adaptive_pomdps.environments.Simulator`): domain to train interactions from
             conf: configurations from program input (`network_size`, `learning_rate` and `dropout_rate`)

        """

        POBNRLogger.__init__(self)

        # domain knowledge
        self.domain_action_space = domain.action_space
        self.domain_obs_space = domain.observation_space

        assert isinstance(
            self.domain_obs_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {domain}"
        assert isinstance(
            domain.state_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {domain}"

        self.sample_domain_start_state = domain.sample_start_state

        self.domain_reward = domain.reward
        self.domain_terminal = domain.terminal

        self.domain_state_to_string = domain.state_to_string
        self.domain_action_to_string = domain.action_to_string
        self.domain_observation_to_string = domain.observation_to_string

        self._models = [
            create_dynamics_model(domain, conf) for _ in range(conf.num_nets)
        ]

        self._theta_updates = create_model_updates(conf)

    @property
    def state_space(self) -> Space:
        """ No method should want to know `this` state space, raises error """
        raise NotImplementedError

    def __len__(self):
        return len(self._models)

    @property
    def action_space(self) -> ActionSpace:
        """returns `this` actions space

        Args:

        RETURNS (`general_bayes_adaptive_pomdps.environments.ActionSpace`):

        """
        return self.domain_action_space

    @property
    def observation_space(self) -> DiscreteSpace:
        """returns `this` observation space

        Args:

        RETURNS (`general_bayes_adaptive_pomdps.misc.Space`):

        """
        assert isinstance(
            self.domain_obs_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {self.domain_obs_space}"
        return self.domain_obs_space

    def sample_start_state(self) -> "AugmentedState":
        """returns a sample initial (internal) state and some neural network

        Args:

        RETURNS (`AugmentedState`):

        """
        return self.AugmentedState(
            self.sample_domain_start_state(),
            copy.deepcopy(np.random.choice(self._models)),
        )

    def update_theta(
        self,
        theta: DynamicsModel,
        state: np.ndarray,
        action: np.ndarray,
        next_state: np.ndarray,
        observation: np.ndarray,
    ) -> None:
        """applies all parameters updates on theta, given transition

        Construction figures out what updates are applied onto the parameters
        ``theta`` during a step in the GBA-POMDP. Here, those updates are
        applied given the (``state``, ``action``, ``next_state``,
        ``observation``) transition.

        These updates are applied in place, thus ``theta`` is updated

        Args:
             theta: (`DynamicsModel`):
             state: (`np.ndarray`):
             action: (`np.ndarray`):
             next_state: (`np.ndarray`):
             observation: (`np.ndarray`):

        RETURNS (`None`):

        """
        for update in self._theta_updates:
            update(theta, state, action, next_state, observation)

    @staticmethod
    def simulate_domain_step(
        theta: DynamicsModel, domain_state: np.ndarray, action: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Performs simulation step of the domain state

        Uses ``theta`` as theta to simulate a step in the domain given the
        ``domain_state`` and ``action``. Returns the resulting new domain state
        and observation (both ``np.ndarray``)

        Args:
             theta: (`DynamicsModel`): model parameters
             domain_state: (`np.ndarray`): state at t
             action: (`int`): action at t

        RETURNS (`Tuple[np.ndarray, np.ndarray]`): (s, o) at t+1

        """

        # use theta to generate a step
        return theta.simulation_step(domain_state, action)

    def simulation_step(self, state: AugmentedState, action: int) -> SimulationResult:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP:

        - updates domain state according to :meth:`~simulate_domain_step`
        - updates theta according to :meth:`~update_theta`
        - COPIES MODEL, if you do not care about modifying ``state``, see :meth:`simulation_step_inplace`

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`general_bayes_adaptive_pomdps.environments.SimulationResult`):

        """

        # ``simulation_step_inplace`` modifies the model in ``state``, copy and store it
        old_model = copy.deepcopy(state.model)

        # the actual step
        ret = self.simulation_step_inplace(state, action)

        # both ``state`` and the state in ``ret`` refer to the same updated model
        # reset the original model to the incoming copied one
        state.model = old_model

        return ret

    def simulation_step_inplace(
        self, state: AugmentedState, action: int
    ) -> SimulationResult:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP:

        - updates domain state according to :meth:`~simulate_domain_step`
        - updates theta according to :meth:`~update_theta`
        - MODIFIES ``state``

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`general_bayes_adaptive_pomdps.environments.SimulationResult`):

        """

        new_domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        # update model of state in place
        self.update_theta(
            state.model, state.domain_state, action, new_domain_state, obs
        )

        # return new state with reference to old model
        return SimulationResult(self.AugmentedState(new_domain_state, state.model), obs)

    def simulation_step_without_updating_theta(
        self, state: AugmentedState, action: int
    ) -> SimulationResult:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP
        without actually updating the theta parameters in ``state``:

        - updates domain state according to :meth:`~simulate_domain_step`
        - updates theta according to :meth:`~update_theta`

        Args:
             state: (`AugmentedState`): incoming state
             action: (`int`): action

        RETURNS (`general_bayes_adaptive_pomdps.environments.SimulationResult`):

        """

        new_domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        return SimulationResult(self.AugmentedState(new_domain_state, state.model), obs)

    def reward(
        self, state: AugmentedState, action: int, new_state: AugmentedState
    ) -> float:
        """the reward function of the underlying environment

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             new_state: (`AugmentedState`):

        RETURNS (`float`): the reward of the transition

        """
        return self.domain_reward(state.domain_state, action, new_state.domain_state)

    def terminal(
        self, state: AugmentedState, action: int, new_state: AugmentedState
    ) -> bool:
        """the termination function of the underlying environment

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             new_state: (`AugmentedState`):

        RETURNS (`bool`): whether the transition is terminal

        """
        return self.domain_terminal(state.domain_state, action, new_state.domain_state)

    def reset(
        self,
        train_net_f: Callable[[DynamicsModel], None],
        learning_rate: float,
        online_learning_rate: float,
    ) -> None:
        """resets the ensemble and re-trains them according

        After training, the learning rate is reset to `online_learning_rate`
        for online updating

        Args:
             train_net_f: (`Callable[[DynamicsModel], None]`):
             learning_rate: (`float`): learning rate to learn models with
             online_learning_rate: (`float`): learning rate to set to after learning

        RETURNS (`None`):

        """

        for i, model in enumerate(self._models):

            self.log(
                POBNRLogger.LogLevel.V1,
                f"Training Dynamics Neural Ensemble member {i+1}/{len(self._models)}",
            )

            model.reset()
            model.set_learning_rate(learning_rate)
            train_net_f(model)

            # after training models we use this low learning rate
            # for online updates
            model.set_learning_rate(online_learning_rate)

    def state_to_string(self, state: AugmentedState) -> str:
        """Returns a string representation of the `state`

        Prints underlying state

        Args:
            state (`AugmentedState`):

        Returns:
            `str`:
        """
        return f"Augmented state {self.domain_state_to_string(state.domain_state)}"

    def action_to_string(self, action: int) -> str:
        """Returns a string representation of the `action`

        Prints domain action

        Args:
            action (`int`):

        Returns:
            `str`:
        """
        return self.domain_action_to_string(action)

    def observation_to_string(self, observation: np.ndarray) -> str:
        """Returns a string representation of the `observation`

        Prints domain observation

        Args:
            observation (`np.ndarray`):

        Returns:
            `str`:
        """
        return self.domain_observation_to_string(observation)
