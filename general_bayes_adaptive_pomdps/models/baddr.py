"""Contains the `BADDr` instance of the general BA-POMDP"""
import copy
import random
from functools import partial
from typing import List, NamedTuple, Tuple

import numpy as np
from typing_extensions import Protocol

from general_bayes_adaptive_pomdps.core import (
    ActionSpace,
    DomainSimulationStep,
    DomainStatePrior,
    GeneralBAPOMDP,
    RewardFunction,
    SimulationResult,
    TerminalFunction,
    TerminalState,
    Transition,
)
from general_bayes_adaptive_pomdps.misc import DiscreteSpace
from general_bayes_adaptive_pomdps.models.neural_networks import pytorch_api
from general_bayes_adaptive_pomdps.models.neural_networks.neural_pomdps import (
    DynamicsModel,
    get_optimizer_builder,
)


class TransitionSampler(Protocol):
    """type to represent a domain transition sampler"""

    def __call__(self) -> Transition:
        """plain call to generate s,a,s',o sample"""


def train_from_samples(
    theta: DynamicsModel,
    sampler: TransitionSampler,
    num_epochs: int,
    batch_size: int,
) -> float:
    """trains a theta with data uniformly sampled from (S,A) space

    Performs `num_epochs` updates of size `batch_size` by sampling from sampler

    Args:
         theta: (`general_bayes_adaptive_pomdps.models.neural_networks.neural_pomdps.DynamicsModel`):
         sampler: (`TransitionSampler`): method to sample transitions from
         num_epochs: (`int`): number of batch updates
         batch_size: (`int`): size of a batch update

    RETURNS (`float`): loss
    """
    loss = 0.0

    for _ in range(num_epochs):
        states, actions, new_states, observations = zip(
            *[sampler() for _ in range(batch_size)]
        )
        loss += theta.batch_update(
            np.array(states),
            np.array(actions),
            np.array(new_states),
            np.array(observations),
        )

    return loss


def sample_transitions_uniform(
    state_space: DiscreteSpace,
    action_space: ActionSpace,
    domain_simulation_step: DomainSimulationStep,
) -> Transition:
    """Samples transitions uniformly from simulator

    Samples state-action pairs uniformly from the domain (the next state
    and observations are generated by the simulator)

    Args:
        state_space (`DiscreteSpace`):
        action_space (`ActionSpace`):
        domain_simulation_step

    Returns:
        `Transition`: transition (s,a,s',o)
    """

    while True:
        try:
            state = state_space.sample()
            action = action_space.sample_as_int()

            next_state, observation = domain_simulation_step(state, action)

            return Transition(state, action, next_state, observation)

        except TerminalState:
            continue


def create_dynamics_model(
    state_space: DiscreteSpace,
    action_space: ActionSpace,
    observation_space: DiscreteSpace,
    optimizer: str,
    learning_rate: float,
    network_size: int,
    batch_size: int,
    dropout_rate: float = 0.0,
) -> DynamicsModel:
    """factory function for creating `DynamicsModel`

    The `DynamicsModel` consists of a transition and observation model. These
    are constructed here given the dimensions and training parameters.

    Args:
        state_space (`DiscreteSpace'):
        action_space (`ActionSpace'):
        observation_space (`DiscreteSpace'):
        optimizer (`str`): in ["SGD" or "Adam"], sets type of optimizer
        learning_rate (`float`):
        network_size (`int`): number of nodes in each fully connected layer
        batch_size (`int`):
        dropout_rate (`float`): ratio of dropping nodes (default is 0)

    Returns:
        `DynamicsModel`:
    """

    optimizer_builder = get_optimizer_builder(optimizer)

    obs_model = DynamicsModel.ONet(
        state_space,
        action_space,
        observation_space,
        optimizer_builder,
        learning_rate,
        network_size,
        dropout_rate,
    )

    trans_model = DynamicsModel.TNet(
        state_space,
        action_space,
        optimizer_builder,
        learning_rate,
        network_size,
        dropout_rate,
    )

    return DynamicsModel(
        state_space,
        action_space,
        batch_size,
        trans_model,
        obs_model,
    )


class ModelUpdate(Protocol):
    """Defines the signature of a model update function

    These functions are used to **augment** the importance sampling update.
    I.e. this function will enable the model to be updated

    """

    def __call__(
        self,
        model: DynamicsModel,
        state: np.ndarray,
        action: int,
        next_state: np.ndarray,
        observation: np.ndarray,
    ) -> None:
        pass


def get_model_freeze_setting(
    freeze_model: str,
) -> DynamicsModel.FreezeModelSetting:
    """returns the model freeze setting given configuration string

    Args:
         freeze_model: (`str`):

     Returns:
         `general_bayes_adaptive_pomdps.models.neural_networks.neural_pomdps.DynamicsModel.FreezeModelSetting`:

    """
    if not freeze_model:
        return DynamicsModel.FreezeModelSetting.FREEZE_NONE

    if freeze_model == "T":
        return DynamicsModel.FreezeModelSetting.FREEZE_T

    if freeze_model == "O":
        return DynamicsModel.FreezeModelSetting.FREEZE_O

    raise ValueError("Wrong value given to freeze model argument")


def backprop_update(
    model: DynamicsModel,
    state: np.ndarray,
    action: int,
    next_state: np.ndarray,
    observation: np.ndarray,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """A type of belief update: applies a simple backprop call to the model

    Args:
         model: (`DynamicsModel`):
         state: (`np.ndarray`):
         action: (`int`):
         next_state: (`np.ndarray`):
         observation: (`np.ndarray`):
         freeze_model_setting: (`FreezeModelSetting`)

    Returns:
        None
    """

    # `batch_update` expects batch_size x ... size. [None] adds a dimension
    model.batch_update(
        state[None],
        np.array([action]),
        next_state[None],
        observation[None],
        freeze_model_setting,
    )


def replay_buffer_update(
    model: DynamicsModel,
    state: np.ndarray,
    action: int,
    next_state: np.ndarray,
    observation: np.ndarray,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """will add transition to model and then invoke a `self learn` step

    Args:
         model: (`DynamicsModel`):
         state: (`np.ndarray`):
         action: (`np.ndarray`):
         next_state: (`np.ndarray`):
         observation: (`np.ndarray`):
         freeze_model_setting: (`FreezeModelSetting`)

    Returns:
        None
    """

    model.add_transition(state, action, next_state, observation)
    model.self_learn(freeze_model_setting)


def perturb_parameters(
    model: DynamicsModel,
    state: np.ndarray,
    action: int,
    next_state: np.ndarray,
    observation: np.ndarray,
    stdev: float,
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
) -> None:
    """A type of belief update: applies gaussian noise to model parameters

    Args:
         model: (`DynamicsModel`):
         _: (`np.ndarray`): ignored
         __: (`np.ndarray`): ignored
         ___: (`np.ndarray`): ignored
         _____: (`np.ndarray`): ignored
         stdev: (`float`): the standard deviation of the applied noise
         freeze_model_setting: (`FreezeModelSetting`)

    RETURNS (`None`):

    """
    model.perturb_parameters(stdev, freeze_model_setting)


def create_model_updates(
    freeze_model_setting: DynamicsModel.FreezeModelSetting,
    backprop: bool,
    replay_update: bool,
    perturb_stdev: float,
) -> List[ModelUpdate]:
    """create_model_updates.

    Args:
        freeze_model_setting (`DynamicsModel.FreezeModelSetting`):
        backprop (`bool`): whether to do backpropagation
        replay_update (`bool`): whether to use a replay buffer to train on
        perturb_stdev (`float`): amount to perturb parameters by during update

    Returns:
        `List[ModelUpdate]`:
    """
    updates: List[ModelUpdate] = []

    if backprop:
        updates.append(
            partial(backprop_update, freeze_model_setting=freeze_model_setting)
        )
    if replay_update:
        updates.append(
            partial(
                replay_buffer_update,
                freeze_model_setting=freeze_model_setting,
            )
        )
    if perturb_stdev:
        updates.append(
            partial(
                perturb_parameters,
                stdev=perturb_stdev,
                freeze_model_setting=freeze_model_setting,
            )
        )

    return updates


class BADDrState(NamedTuple):
    """A state containing (POMDP state, POMDP dynamics posterior)"""

    domain_state: np.ndarray
    model: DynamicsModel


class BADDr(GeneralBAPOMDP[BADDrState]):
    """The GBA-POMDP where the belief over the model is parameterized by dropout networks"""

    def __init__(
        self,
        action_space: ActionSpace,
        observation_space: DiscreteSpace,
        sample_domain_start_state: DomainStatePrior,
        reward_function: RewardFunction,
        terminal_function: TerminalFunction,
        prior_models: List[DynamicsModel],
        model_update: List[ModelUpdate],
        use_gpu: bool = False,
    ):
        """Creates `BADDr`

        To create `model_update` consider :func:`create_model_updates`.
        To create `prior_models` consider :func:`create_dynamics_model`.

        Args:
            state_space (`DiscreteSpace'):
            action_space (`ActionSpace'):
            observation_space (`ObservationSpace'):
            sample_domain_start_state (`Callable'):
            reward_function (`RewardFunction`):
            terminal_function (`TerminalFunction`):
            prior_models (`List[DynamicsModel]`): list of models to start with a-priori
            model_update (`List[ModelUpdate]`): list of operations to perform to update a model posterior
            use_gpu (`bool`): whether to use the GPU

        """

        pytorch_api.set_device(use_gpu)

        # domain knowledge
        self.domain_action_space = action_space
        self.domain_obs_space = observation_space
        self.sample_domain_start_state = sample_domain_start_state
        self.domain_reward = reward_function
        self.domain_terminal = terminal_function

        self._models = prior_models
        self._theta_updates = model_update

    def __len__(self):
        return len(self._models)

    @property
    def action_space(self) -> ActionSpace:
        """returns `this` actions space

        Args:

        RETURNS (`general_bayes_adaptive_pomdps.core.ActionSpace`):

        """
        return self.domain_action_space

    @property
    def observation_space(self) -> DiscreteSpace:
        """returns `this` observation space

        Args:

        RETURNS (`general_bayes_adaptive_pomdps.misc.Space`):

        """
        assert isinstance(
            self.domain_obs_space, DiscreteSpace
        ), f"current limited to learning discrete POMDPs, not {self.domain_obs_space}"
        return self.domain_obs_space

    def sample_start_state(self) -> BADDrState:
        """returns a sample initial (internal) state and some neural network

        Args:

        RETURNS (`AugmentedState`):
        """
        return BADDrState(
            self.sample_domain_start_state(),
            copy.deepcopy(random.choice(self._models)),
        )

    @staticmethod
    def update_theta(
        theta: DynamicsModel,
        state: np.ndarray,
        action: np.ndarray,
        next_state: np.ndarray,
        observation: np.ndarray,
        theta_updates: List[ModelUpdate],
    ) -> None:
        """applies all parameters updates on theta, given transition

        Construction figures out what updates are applied onto the parameters
        ``theta`` during a step in the GBA-POMDP. Here, those updates are
        applied given the (``state``, ``action``, ``next_state``,
        ``observation``) transition.

        These updates are applied in place, thus ``theta`` is updated

        Args:
             theta: (`DynamicsModel`):
             state: (`np.ndarray`):
             action: (`np.ndarray`):
             next_state: (`np.ndarray`):
             observation: (`np.ndarray`):
             theta_updates: (`List[ModelUpdate]`): list of updates to perform on ``theta``

        RETURNS (`None`):

        """
        for update in theta_updates:
            update(theta, state, action, next_state, observation)

    @staticmethod
    def simulate_domain_step(
        theta: DynamicsModel, domain_state: np.ndarray, action: int
    ) -> SimulationResult:
        """Performs simulation step of the domain state

        Uses ``theta`` as theta to simulate a step in the domain given the
        ``domain_state`` and ``action``. Returns the resulting new domain state
        and observation (both ``np.ndarray``)

        Args:
             theta: (`DynamicsModel`): model parameters
             domain_state: (`np.ndarray`): state at t
             action: (`int`): action at t

        RETURNS (`SimulationResult`): (s, o) at t+1

        """

        # use theta to generate a step
        return theta.simulation_step(domain_state, action)

    def simulation_step(
        self, state: BADDrState, action: int, optimize: bool = False
    ) -> Tuple[BADDrState, np.ndarray]:
        """Performs simulation step

        Simulates performing an ``action`` in ``state`` of the GBA-POMDP. Part
        of :class:`GeneralBAPOMDP` protocol.

        - updates domain state according to :meth:`simulate_domain_step`
        - updates theta according to :meth:`update_theta`
        - COPIES MODEL, if you do not care about modifying ``state``, see :meth:`simulation_step_inplace`

        """

        next_domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        next_model = state.model if optimize else copy.deepcopy(state.model)
        self.update_theta(
            next_model,
            state.domain_state,
            action,
            next_domain_state,
            obs,
            self._theta_updates,
        )

        return BADDrState(next_domain_state, next_model), obs

    def reward(self, state: BADDrState, action: int, next_state: BADDrState) -> float:
        """the reward function of the underlying domain

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             next_state: (`AugmentedState`):

        RETURNS (`float`): the reward of the transition

        """
        return self.domain_reward(state.domain_state, action, next_state.domain_state)

    def terminal(self, state: BADDrState, action: int, next_state: BADDrState) -> bool:
        """the termination function of the underlying domain

        Args:
             state: (`AugmentedState`):
             action: (`int`):
             next_state: (`AugmentedState`):

        RETURNS (`bool`): whether the transition is terminal

        """
        return self.domain_terminal(state.domain_state, action, next_state.domain_state)

    def domain_simulation_step(
        self, state: BADDrState, action: int
    ) -> SimulationResult:
        """Performs the domain state part of the step in the GBA-POMDP

        Leaves the model alone

        Note: part of `GBAPOMDP` protocol
        """
        domain_state, obs = self.simulate_domain_step(
            state.model, state.domain_state, action
        )

        next_state = BADDrState(domain_state, state.model)

        return SimulationResult(next_state, obs)

    def model_simulation_step(
        self,
        to_update: BADDrState,
        prev_state: BADDrState,
        action: int,
        next_state: BADDrState,
        obs: np.ndarray,
        optimize: bool = False,
    ) -> BADDrState:
        """Performs the model part of the step in the GBA-POMDP

        Leaves the state alone
        If ``optimize`` is true, will modify model in ``to_update``
        Note: part of `GBAPOMDP` protocol
        """
        model = to_update.model if optimize else copy.deepcopy(to_update.model)

        self.update_theta(
            model,
            prev_state.domain_state,
            action,
            next_state.domain_state,
            obs,
            self._theta_updates,
        )
        return BADDrState(to_update.domain_state, model)
